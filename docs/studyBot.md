# Study Bot - studyBot.py

## Contents

- [Description](#description)
- [Usage](#usage)
- [Code Walkthrough](#code-walkthrough)
	- [Imports](#imports)
	- [Initialize variables and API configurations](#initialize-variables-and-api-configurations)
	- [Audio recording: ```recordQuestion()```](#audio-recording-recordquestion)
	- [Object recognition: ```lookForObjects()```](#object-recognition-lookforobjects)
	- [Answer generation: ```sendMessage()```](#answer-generation-sendmessage)
	- [Text to speech: ```convertTTS()``` and ```streamAnswer()```](#text-to-speech-converttts-and-streamanswer)
	- [Use of multithreading and main](#use-of-multithreading-and-main)
	- [Conversation handling](#conversation-handling)
- [Final notes](#final-notes)

## Description

This script contains all of StudyBot's functionalities, and when it is run as a standalone script, it will act as a command-line interface for the user to interact with the bot. This is used mostly for testing new features and debugging. Note that this documentation was created with the intent of helping a student-level developer understand the code to the point of being able to contribute to the project, so it may give explanations for things that may seem obvious or trivial to some.

## Usage

It is recommended to use [Python 3.9.9](https://www.python.org/downloads/release/python-399/), since the ```whisper``` library may present compatibility issues with newer versions of Python. 

If you want to avoid having to remove your current Python installation, you may want to set up a virtual enviroment to use this specific version of Python. To install the required dependencies, run the following command:

```bash
pip install -r requirements.txt
```

To use this script, simply run the following command within the root directory of the project. Consider that it will almost immediately start listening for the user's question and looking for objects, so it is recommended to have the educational materials and the question ready before running the script.

```bash
python src/studyBot.py
```

## Code Walkthrough

### Imports

```python
import openai
import whisper
from pydub import AudioSegment
from pydub.playback import play as pydubPlay
import io
from typing import Iterator
import pyaudio
import wave
from pathlib import Path
from elevenlabs import generate, set_api_key
import cv2
import numpy as np
import time
import threading
import keyboard
import credentials
```
- ```openai```: Used to access the API to access GPT-3.5
- ```whisper```: Used to access the API for the speech-to-text conversion
- ```pydub```: Used to stream the audio as it is generated by the text-to-speech conversion service
- ```io```: Used for in-memory input/output operations, treating data as bytes. Provides a stream interface for working with binary data in memory to allow audio streaming
- ```pyaudio```: Abstracts the complexities of audio input/output operations, allowing us to easily record audio from the microphone
- ```wave```: Used to save audio recordings in the ```.wav``` format
- ```pathlib```: Allows us to easily work with file paths, used to delete the audio recordings after they are no longer needed
- ```elevenlabs```: Used to access the API for the text-to-speech conversion service
- ```cv2```: Computer vision library used to detect the physical educational material that the user is holding using color
- ```numpy```: Used to work with arrays, which are used to store the pixel data of the images
- ```time```: Allows the use of timed functions, used to control how long the bot will look for objects
- ```threading```: Used to perform multiple tasks at the same time using multithreading, allowing the bot to look for objects while the user is asking their question
- ```keyboard```: Used to detect keyboard inputs to control the bot
- ```credentials```: Contains the API keys as variables for the services used by the bot. Note that this file is not included in the repository for security reasons, and must be created manually by the contributor/developer based on the template provided in ```src/credentials-template.py```
---
### Initialize variables and API configuration

Global variables are used to avoid using input parameters and return values for most of our functions, as this complicates things when the functions are called asynchronously using multithreading.

Here is where the API keys should be set after creating the ```credentials.py``` file. You can obtain an API key from OpenAI [here](https://platform.openai.com/account/api-keys), and an API key from ElevenLabs [here](https://docs.elevenlabs.io/api-reference/quick-start/authentication). These sites also provide additional information on how to use the APIs.

```python
global objects
global question
global answer
objects = ''
question = ''
answer = ''

GPT_MODEL = 'gpt-3.5-turbo'

# Credentials
openai.api_key = credentials.openAiKey
set_api_key(credentials.elevenLabsKey)
```

Here you can set up the information sources that GPT will use to answer questions, as well as customize the behavioral guidelines that it will follow when interacting with the user.

```python
humanBodySource = """
Stomach: The stomach is an organ located in...
"""

biochemSource = """
There is no source for this topic. Inform the user that this topic is not supported yet.
"""

# Behavioral guidelines for conversation
instructions = """
Try to use the information below to help the user...
"""
```
---
### Audio recording: ```recordQuestion()```

This functions records audio for 5 seconds and saves it to a file called ```question.wav``` and sends the file through the Whisper API to create a transcription of the audio. If needed, extend the duration of the recording by modifying the ```RECORD_SECONDS``` variable.

While you could select the language Whisper should expect as a parameter for the ```model.transcribe()``` function, ignoring this parameter as below will make Whisper detect the language automatically, which is a desireable feature to avoid having to select the language every time the bot is used.

>Note: Whisper requires the ```ffmpeg``` command line tool to be installed in your system to function.

```python
def recordQuestion():
	# ---------------- Audio Recording ----------------
	global question
	audio = pyaudio.PyAudio() # Initialize PyAudio
	# Open audio stream for recording
	stream = audio.open(format = FORMAT, channels = CHANNELS, 
						rate = RATE, input = True, 
						frames_per_buffer = CHUNK)
	frames = []

	# Record audio stream in chunks
	for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
		data = stream.read(CHUNK)
		frames.append(data)

	# Stop and close audio stream
	stream.stop_stream()
	stream.close()
	audio.terminate()

	# Save recording as WAV
	wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
	wf.setnchannels(CHANNELS)
	wf.setsampwidth(audio.get_sample_size(FORMAT))
	wf.setframerate(RATE)
	wf.writeframes(b''.join(frames))
	wf.close()

	# ---------------- STT Conversion -----------------
	model = whisper.load_model('base')
	result = model.transcribe('question.wav', fp16 = False)
	question = result['text']
	
	# Delete audio file
	Path('question.wav').unlink()
```
---
### Object recognition: ```lookForObjects()```

This function uses [OpenCV](https://opencv.org/) to detect objects of a certain color in the user's camera feed. It will look for objects for one second, and if it finds any, it will modify the objects variable to contain a list of the objects it found. If it doesn't find any objects, the variable will contain the default message, set at the start of the function.

For each of the objects you wish to identify, set a lower and upper bound with the lightest and darkest values for the color of the object in the HSV color space. You may use a color picker tool to obtain these values. Consider that the effectiveness of these values will depend on the lighting conditions of the environment, color accuracy of the camera, white balance, and other factors.

>Note: For OpenCV, the H (hue) value is represented as a value between 0 and 180, instead of the standard 0 to 360, and the S (saturation) and V (vibrancy) values are represented as values between 0 and 255 instead of 0 to 100.

```python
def lookForObjects():
	global objects
	objects = 'User is not holding any objects'

	# Capture video
	cam = cv2.VideoCapture(0) # Use 0 for default camera

	# Start timer
	startTime = time.time()
	elapsedTime = 0

	# Color ranges
	stomachLower = np.array([90,  80,         1         ], np.uint8)
	stomachUpper = np.array([120, 255,        255       ], np.uint8)
	colonLower = np.array(	[9,   255 * 0.55, 255 * 0.35], np.uint8)
	colonUpper = np.array(	[18,  255,        255       ], np.uint8)
	liverLower = np.array(	[38,  225 * 0.22, 255 * 0.38], np.uint8)
	liverUpper = np.array(	[41,  255,        255       ], np.uint8)
	brainLower = np.array(	[168, 255 * 0.50, 255 * 0.40], np.uint8)
	brainUpper = np.array(	[168, 255,        255       ], np.uint8)
	kidneyLower = np.array(	[26,  255 * 0.60, 255 * 0.49], np.uint8)
	kidneyUpper = np.array(	[26,  255,        255       ], np.uint8)
	heartLower = np.array(	[179, 255 * 0.50, 255 * 0.45], np.uint8)
	heartUpper = np.array(	[179, 255 * 0.97, 255 * 0.69], np.uint8)

	while elapsedTime < 1:

		_, imageFrame = cam.read()

		# Convert frame from BGR color space to HSV
		hsvFrame = cv2.cvtColor(imageFrame, cv2.COLOR_BGR2HSV)

		# Create masks for each organ
		colonMask = cv2.inRange(hsvFrame, colonLower, colonUpper)
		liverMask = cv2.inRange(hsvFrame, liverLower, liverUpper)
		stomachMask = cv2.inRange(hsvFrame, stomachLower, stomachUpper)
		brainMask = cv2.inRange(hsvFrame, brainLower, brainUpper)
		kidneyMask = cv2.inRange(hsvFrame, kidneyLower, kidneyUpper)
		heartMask = cv2.inRange(hsvFrame, heartLower, heartUpper)
```

Here, we create masks for each organ. A mask is a binary image where the pixels that match the color range are white, and the pixels that don't are black. We then use the masks to find the contours of the objects in the image.

In order to reduce the likelihood of false negatives, we perform a small amount of image processing. A morphological transformation (operation that alters the pixels of an image based on the surrounding pixels) called dilation is applied, which enlarges the boundaries of objects in a binary image. It works by sliding a structuring element (also called a kernel) over the image and replacing each pixel with the maximum pixel value within the kernel's neighborhood. The kernel is a small matrix that defines the shape of the dilation operation. In this Python project, a 5x5 square-shaped kernel (filled with ones) is used for most organs, but a more aggressive 12x12 square-shaped kernel is used for the kidney. 

This technique helps to fill holes and gaps in the binary masks for each organ, making the objects more solid and continuous. It helps to increase the size of the colored regions in the binary masks, which can improve the script's ability to detect the corresponding colored objects in the image. Dilation is especially useful when the objects of interest have small gaps or are not completely filled, as it bridges those gaps and creates more robust masks.

The bitwise `AND` operation is used in combination with binary masks to extract the regions of interest from the original image. The operation involves comparing each pixel of the image with the corresponding pixel of the mask. If both pixels are non-zero (i.e., white in the binary mask), the resulting pixel in the output image retains its original color value; otherwise, it becomes black.

```python
		# Create a 5x5 square-shaped filter called kernel
		# Filter is filled with ones and will be used for morphological 
		# transformations such as dilation for better detection
		kernel = np.ones((5, 5), 'uint8')

		# For colon
		# Dilate mask: Remove holes in the mask by adding pixels to the 
		# boundaires of the objects in the mask
		colonMask = cv2.dilate(colonMask, kernel)
		# Apply mask to frame by using bitwise AND operation
		resColon = cv2.bitwise_and(imageFrame, imageFrame, mask = colonMask)

		# For liver
		liverMask = cv2.dilate(liverMask, kernel)
		resliver = cv2.bitwise_and(imageFrame, imageFrame, mask=liverMask)

		# For stomach
		stomachMask = cv2.dilate(stomachMask, kernel)
		resStomach = cv2.bitwise_and(imageFrame, imageFrame, mask=stomachMask)

		# For brain
		brainMask = cv2.dilate(brainMask, kernel)
		resBrain = cv2.bitwise_and(imageFrame, imageFrame, mask=brainMask)

		# For heart
		heartMask = cv2.dilate(heartMask, kernel)
		resHeart = cv2.bitwise_and(imageFrame, imageFrame, mask=heartMask)

		# For kidney use a more aggressive kernel for dilation
		kidneyMask = cv2.dilate(kidneyMask, np.ones((12, 12), 'uint8'))
		resKidney = cv2.bitwise_and(imageFrame, imageFrame, mask=kidneyMask)
```

We create a contour around the zone that matches the color range of the object. The binary masks generated earlier are used as the input to the ```cv2.findContours()``` function to find contours in the image that correspond to the color range of the object.

The function returns a list of contours and a hierarchy that represents the relationships between the contours. The code iterates over each contour found in the previous step using a for loop. For each contour, the area of the contour is found and then compared to a predefined threshold.

If the area is greater than the threshold, it indicates that the detected region is significant enough to be considered a positive detection for the object. If the area of the contour exceeds the threshold, the code proceeds to annotate the name of the object to the list, while avoiding repeats. This operation is repeated for each object of the set.

```python
		# Create a contour around the zone that matches the color range
		contours, hierarchy = cv2.findContours(colonMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		# For each countour, check if the area is greater than the threshold
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				# Append the name of the model to the list of objects
				if 'colon' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'colon'
					else:
						objects = objects + ', colon'

		contours, hierarchy = cv2.findContours(liverMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				if 'liver' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'liver'
					else:
						objects = objects + ', liver'

		contours, hierarchy = cv2.findContours(stomachMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 1400:
				if 'stomach' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'stomach'
					else:
						objects = objects + ', stomach'

		contours, hierarchy = cv2.findContours(brainMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				if 'brain' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'brain'
					else:
						objects = objects + ', brain'
		
		contours, hierarchy = cv2.findContours(heartMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				if 'heart' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'heart'
					else:
						objects = objects + ', heart'

		contours, hierarchy = cv2.findContours(kidneyMask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
		for pic, contour in enumerate(contours):
			area = cv2.contourArea(contour)
			if area > 500:
				if 'kidney' not in objects:
					if objects == 'User is not holding any objects':
						objects = 'kidney'
					else:
						objects = objects + ', kidney'
```

To calibrate this method of object detection to a specific set of models, it is recommended to use the script located in ```ObjectIdentification\ColorDetection.py``` to find the color ranges that work best for the set of models and test using different area thresholds.

---
### Answer generation: ```sendMessage()```

This function simply takes a message list as input and sends it to the OpenAI API. This uses the chat completions endpoint to allow follow up questions to previous messages. The `temperature` parameter is used to control the randomness of the response. A lower temperature value will result in more predictable responses, as the model is said to take a more objective and factual approach to the response.

After the answer is extracted from the JSON response, it is appended to the message list and returned. Note that the answer is saved as a dictionary with the role of the message (assistant or user), as this is important information for the model to understand who sent which message when generating a future response.

```python
def sendMessage(messageList: any):
	# Send prompt to GPT
	response = openai.ChatCompletion.create(
		messages = messageList,
		model = GPT_MODEL, 
		temperature = 0.2
	)
	# print(response)
	_answer = response['choices'][0]['message']['content']

	# Add the response to the message list
	messageList.append({'role': 'assistant', 'content': _answer})
```
---
### Text to speech: ```convertTTS()``` and ```streamAnswer()```

To convert GPT's text answer into speech, call the Elevenlabs API to generate the audio. Here, the ```eleven_multilingual_v1``` model is used, which automatically detects the language of the input text and generates the audio in the same language as long as it is supported. The ```stream``` parameter is set as True to play the audio as it is being generated, instead of waiting for the entire text to be converted to begin audio playback. The audio is streamed to the ```streamAnswer()``` function, which plays the audio using the ```pydub``` library.

>Note: While there is already a function in the Elevenlabs library to directly stream audio, we use our own function since the library's function requires the use of ```mpv```, a command line media player. While it is possibly the best option for a CLI-only project, this is not ideal for our specific use case, since we want non-technical users to be able to run the program without having to install too many additional dependencies.

```python
def streamAnswer(audioStream: Iterator[bytes]) -> bytes:
    audioOutput = b""
    for chunk in audioStream:
        if chunk is not None:
            audioOutput += chunk

    audioSegment = AudioSegment.from_file(io.BytesIO(audioOutput), format="mp3")
    pydubPlay(audioSegment)

def convertTTS(text: str):
	audioOutput = generate(text = text, model = 'eleven_multilingual_v1', stream = True)
	streamAnswer(audioOutput)
```
---
### Use of multithreading and main

Recording and transcribing the question, as well as detecting the objects held by the user, are both time consuming processes. To avoid having to wait for one to finish before the other can start, we use multithreading to run both processes simultaneously. Create two threads for these processes and wait for them to finish before continuing with the rest of the program.

```python
	objID = threading.Thread(target = lookForObjects)
	audioRec = threading.Thread(target = recordQuestion)

	objID.start()
	print('Looking for objects...\n')
	audioRec.start()
	print('Listening for question...\n')

	objID.join()
	print('Object detection complete.\n')
	print('Objects detected: ' + objects + '\n')
	audioRec.join()
	print('Question recorded.\n')
	print('Question: ' + question + '\n')
```

After the information is gathered, build the initial query. Since this is the very first message sent to GPT, it includes the behavioral guidelines in ```instructions``` and the source material of the set of models. 

```python
	# Build prompt
	query = f"""{instructions}

	Objects held by user: {objects}.
	Question: {question}

	Information: 
	\"\"\"
	{humanBodySource}
	\"\"\"
	"""
```
Notice the role parameters on the messages in the message history. The very first message has the ```system``` role, and serves as some initial context about the task that GPT will be performing in this conversation. 

```python
# Send prompt to GPT
	messageHistory = [
		{'role': 'system', 'content': 'You answer questions in the same 
									   language as the question.'},
		{'role': 'user', 'content': query},
	]

	print('Sending prompt to GPT...\n')
	sendMessage(messageHistory)
	# Get the answer from the last message in the message history
	answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']
	
	if answer != '':
		# print('Answer: ' + answer + '\n\n')
		print('Recieved reply from GPT. \n')

	# Convert answer to audio
	print('Converting answer to audio...\n')
	convertTTS(answer)
```
---
### Conversation handling

After this initial message, the conversation is handled in a loop. The process is pretty straightforward and similar to the first message, with the main difference being the use of the keyboard to trigger the reception of the next question, and the use of the ```sendMessage()``` function to send the question to GPT, appending only the new messages without any need for additional context.

```python
	while True:
		# Wait for the user to press space to ask another question
		print('Press space to ask another question, or press q to quit.\n')

		while True:
			if keyboard.is_pressed(' '):
				print('Preparing for next question, please hold...\n')
				break
			if keyboard.is_pressed('q'):
				print('Exiting program...\n')
				exit()

		# Reset variables
		objects = 'User is not holding any objects'
		question = ''

		# Restart threads

		objID = threading.Thread(target = lookForObjects)
		audioRec = threading.Thread(target = recordQuestion)

		objID.start()
		print('Looking for objects...\n')
		audioRec.start()
		print('Listening for question...\n')

		objID.join()
		print('Object detection complete.\n')
		print('Objects detected: ' + objects + '\n')
		audioRec.join()
		print('Question recorded.\n')
		print('Question: ' + question + '\n')

		# Build prompt with chat history
		# Add last response from GPT to message history
		messageHistory.append({'role': 'assistant', 'content': answer})
		answer = ''

		# Build new prompt and add to chat history
		query = f"""Objects held by user: {objects}.
Question: {question}
"""

		messageHistory.append({'role': 'user', 'content': query})

		# Send prompt to GPT
	
		print('Prompt: ' + query + '\n')

		print('Sending prompt to GPT...\n')

		sendMessage(messageHistory)
		answer = next((msg for msg in reversed(messageHistory) if msg['role'] == 'assistant'), None)['content']

		if answer != '':
			# print('Answer: ' + answer + '\n\n')
			print('Recieved reply from GPT. \n')

		# Convert answer to audio
		print('Converting answer to audio...\n')
		convertTTS(answer)
```
---

## Final notes

There are several improvements that could be made to this script. For example, when entering the conversation handling stage, the program listens for the user to press the space bar or the q key. It does this even when the window is not in focus, which means that if the user presses the space bar while being in a different window, the program will still detect it and continue. This is not ideal, since the user might not be aware that the program is still running, and might accidentally trigger the next question (This has happened several times during development). 

Another improvement would be to use a neural net to detect the objects held by the user. This would be more accurate than the current method, which only uses the color of the object to determine what it is.